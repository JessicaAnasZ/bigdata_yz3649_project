{"cells": [{"cell_type": "code", "execution_count": null, "id": "c240ce0f", "metadata": {"code_folding": []}, "outputs": [], "source": "##pip install jupyter_contrib_nbextensions"}, {"cell_type": "markdown", "id": "3cd7f9db", "metadata": {}, "source": "#### Load data"}, {"cell_type": "code", "execution_count": 41, "id": "ac008280", "metadata": {"code_folding": [0]}, "outputs": [], "source": "#load libraries\nimport pandas as pd \nimport numpy as np \nfrom datetime import datetime\nimport sys\nimport ast\nimport csv\n\n#import nltk\n#from nltk.corpus import stopwords\n#import spacy\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport networkx as nx\nfrom networkx.algorithms.components.connected import connected_components\n\nimport json\nimport dask.bag as db\n\nimport sys\nimport os\n\nsys.path.append(\"..\")\n\nfrom pathlib import Path\n\nimport json\n\n\nfrom itertools import combinations\nfrom collections import Counter\nfrom itertools import chain\nimport random\n\nfrom tqdm.notebook import tqdm, trange\nimport time    # to be used in loop iterations\n\nimport multiprocessing\n\nfrom IPython.core.display import display, HTML\n\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity,cosine_distances\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\n\nfrom itertools import permutations\nfrom pyspark import SparkConf, SparkContext, SQLContext\nimport pyspark\nimport sys\nfrom collections import defaultdict\nfrom graphframes import *"}, {"cell_type": "code", "execution_count": null, "id": "2df5d70a", "metadata": {}, "outputs": [], "source": "## List of categories  http://arxitics.com/help/categories?group=cs"}, {"cell_type": "code", "execution_count": null, "id": "78dd2453", "metadata": {}, "outputs": [], "source": "\"\"\"'cs.AI','cs.CL','cs.CC','cs.CE','cs.CG','cs.GT','cs.CV','cs.CY','cs.CR','cs.DS'\n           ,'cs.DB','cs.DL','cs.DM','cs.DC','cs.DT','cs.FL','cs.GL','cs.GR','cs.AR','cs.HC'\n           ,'cs.IR','cs.IT','cs.LG','cs.LO','cs.MS','cs,MA','cs.MM','cs.NI','cs.NE','cs.NA'\n           ,'cs.OS','cs.OH','cs.PF','cs.PL','cs.RO','cs.SI','cs.SE','cs.SD','cs.SC','cs.SY'\"\"\""}, {"cell_type": "code", "execution_count": 2, "id": "d5865d73", "metadata": {"code_folding": [0]}, "outputs": [], "source": "## set checkpoint\nsc.stop()\nif not os.path.isdir(\"checkpoints\"):\n    os.mkdir(\"checkpoints\")\nconf = SparkConf().setMaster('local').setAppName('connected component')\n\nsc = SparkContext(conf=conf)\nsqlcontext = SQLContext(sc)\nsc.setCheckpointDir('~/Local Disk/checkpoints')"}, {"cell_type": "code", "execution_count": null, "id": "c24ca8aa", "metadata": {"code_folding": [0]}, "outputs": [], "source": "## load dataset to storage function\ndef load_dataset(raw_data_path,categories,year,bucket):\n    records=db.read_text(raw_data_path).map(lambda x:json.loads(x))\n    for category in categories:\n        docs = (records.filter(lambda x:category in x['categories']))\n        extract_latest_version=lambda x:x['versions'][-1][\"created\"]\n        if year!=None:\n            docs=docs.filter(lambda x:int(extract_latest_version(x).split(\" \")[3])>=year)\n\n        get_metadata = lambda x: {'id': x['id'],\n                        'title': x['title'],\n                        'category':x['categories'],\n                        'abstract':x['abstract'],\n                        'version':x['versions'][-1]['created'],\n                        'doi':x[\"doi\"], \n                        'authors_parsed':x['authors_parsed']}\n\n        data=docs.map(get_metadata).to_dataframe().compute()\n\n        ## Creating authors fields by joining first and last nmes in authors_parsed columns.\n        data['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\n\n        print(\"Number of Records Extracted for Category \",category, data.shape[0])\n\n        data.to_json('gs://{}/arxiv_data_cs_field_category_{}.json'.format(bucket,category),orient=\"records\")"}, {"cell_type": "code", "execution_count": 40, "id": "ce3aa08d", "metadata": {"code_folding": []}, "outputs": [], "source": "## setting\nyear=2011\ncategories=['cs.AI','cs.CL','cs.CC','cs.CE','cs.CG','cs.GT','cs.CV','cs.CY','cs.CR','cs.DS'\n           ,'cs.DB','cs.DL','cs.DM','cs.DC']\nraw_data_path='gs://bigdata-homework2-question2-bucket/arxiv-metadata-oai-snapshot.json'\nbucket = \"bigdata-homework2-question2-bucket\"\n\n#load_dataset(raw_data_path=raw_data_path,categories=categories,year=year,bucket=bucket)"}, {"cell_type": "code", "execution_count": null, "id": "bf852eeb", "metadata": {"code_folding": [0]}, "outputs": [], "source": "## cs_field_total\ncategories=['cs.AI','cs.CL','cs.CC','cs.CE','cs.CG','cs.GT','cs.CV','cs.CY','cs.CR','cs.DS'\n           ,'cs.DB','cs.DL','cs.DM','cs.DC','cs.DT','cs.FL','cs.GL','cs.GR','cs.AR','cs.HC'\n           ,'cs.IR','cs.IT','cs.LG','cs.LO','cs.MS','cs,MA','cs.MM','cs.NI','cs.NE','cs.NA'\n           ,'cs.OS','cs.OH','cs.PF','cs.PL','cs.RO','cs.SI','cs.SE','cs.SD','cs.SC','cs.SY']\n\nrecords=db.read_text(raw_data_path).map(lambda x:json.loads(x))\ndocs = (records.filter(lambda x:any(ele in x['categories'] for ele in categories)==True))\nextract_latest_version=lambda x:x['versions'][-1][\"created\"]\nif year!=None:\n    docs=docs.filter(lambda x:int(extract_latest_version(x).split(\" \")[3])>=year)\n\nget_metadata = lambda x: {'id': x['id'],\n                        'title': x['title'],\n                        'category':x['categories'],\n                        'abstract':x['abstract'],\n                        'version':x['versions'][-1]['created'],\n                        'doi':x[\"doi\"], \n                        'authors_parsed':x['authors_parsed']}\n\ndata=docs.map(get_metadata).to_dataframe().compute()\n\n## Creating authors fields by joining first and last nmes in authors_parsed columns.\ndata['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\n\nprint(\"Number of Records Extracted for cs field 2011 \",data.shape[0])\n\ndata.to_json('gs://{}/arxiv_data_cs_field_2011.json'.format(bucket),orient=\"records\")"}, {"cell_type": "markdown", "id": "35d2528b", "metadata": {}, "source": "#### Creating a Co-Author Network"}, {"cell_type": "code", "execution_count": null, "id": "09a78dc9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "there are 6265 clusters in total\n+------------------+-----------+\n|                id|article_num|\n+------------------+-----------+\n|     Abbeel Pieter|        124|\n|     Levine Sergey|        146|\n|      Finn Chelsea|         60|\n|   Haarnoja Tuomas|          9|\n|Rajeswaran Aravind|         14|\n|         Yu Tianhe|         10|\n|    Gupta Abhishek|         25|\n|  Calandra Roberto|         15|\n|    Ebert Frederik|          8|\n|Eysenbach Benjamin|         14|\n+------------------+-----------+\n\nthere are 3079 clusters in total\n+--------------------+-----------+\n|                  id|article_num|\n+--------------------+-----------+\n|         Hovy Eduard|         75|\n|       Metze Florian|         42|\n|       Neubig Graham|        163|\n|    Nakamura Satoshi|         38|\n|           Hu Junjie|         24|\n|Anastasopoulos An...|         44|\n|         Yang Yiming|         32|\n|     Rijhwani Shruti|         13|\n|          Wang Xinyi|         19|\n|     Chaudhary Aditi|         13|\n+--------------------+-----------+\n\nthere are 1467 clusters in total\n+-----------------+-----------+\n|               id|article_num|\n+-----------------+-----------+\n|  Demaine Erik D.|         39|\n|Demaine Martin L.|         10|\n|    Korman Matias|          7|\n|  Hesterberg Adam|          9|\n|     Lynch Jayson|         19|\n|  Bosboom Jeffrey|          8|\n|    Rudoy Mikhail|          8|\n| Coulombe Michael|          3|\n|Hendrickson Dylan|          5|\n|      Asif Sualeh|          3|\n+-----------------+-----------+\n\nthere are 2002 clusters in total\n+-------------------+-----------+\n|                 id|article_num|\n+-------------------+-----------+\n|  De Gersem Herbert|         19|\n|   Sch\u00f6ps Sebastian|         57|\n|       R\u00f6mer Ulrich|         15|\n|Garcia Idoia Cortes|          6|\n| Maciejewski Micha\u0142|          5|\n|     Bortot Lorenzo|          5|\n|       Prioli Marco|          4|\n|  Auchmann Bernhard|          5|\n|     Bontinck Zeger|          6|\n|       Corno Jacopo|          6|\n+-------------------+-----------+\n\nthere are 955 clusters in total\n+-------------------+-----------+\n|                 id|article_num|\n+-------------------+-----------+\n|      Korman Matias|         54|\n|Silveira Rodrigo I.|         22|\n|     Bose Prosenjit|         58|\n|    Mulzer Wolfgang|         52|\n|    L\u00f6ffler Maarten|         44|\n|    Demaine Erik D.|         50|\n|  van Renssen Andr\u00e9|         37|\n|   Aichholzer Oswin|         32|\n| Vogtenhuber Birgit|         29|\n|  Roeloffzen Marcel|         18|\n+-------------------+-----------+\n\nthere are 1332 clusters in total\n+--------------------+-----------+\n|                  id|article_num|\n+--------------------+-----------+\n|         Ba\u015far Tamer|         21|\n|        Bennis Mehdi|         20|\n|          Saad Walid|         69|\n|             Han Zhu|         38|\n|     Poor H. Vincent|         24|\n|        Niyato Dusit|         47|\n| Mandayam Narayan B.|         11|\n|        Semiari Omid|          8|\n|Manshaei Mohammad...|          9|\n|       Sanjab Anibal|         10|\n+--------------------+-----------+\n\nthere are 5404 clusters in total\n+-----------------+-----------+\n|               id|article_num|\n+-----------------+-----------+\n|     Van Gool Luc|        249|\n|       Gu Shuhang|         28|\n|     Timofte Radu|        124|\n| Danelljan Martin|         51|\n|   Ignatov Andrey|         17|\n|    Zhao Tongtong|         14|\n|      Li Chenghua|          8|\n|   S Hrishikesh P|          7|\n|Puthussery Densen|          9|\n|         C Jiji V|          7|\n+-----------------+-----------+\n\nthere are 4046 clusters in total\n+--------------------+-----------+\n|                  id|article_num|\n+--------------------+-----------+\n|      Ferrara Emilio|         34|\n|          Ghosh Rumi|          6|\n|     Lerman Kristina|         40|\n|            Hogg Tad|          5|\n|Kumaraguru Ponnur...|         25|\n|     Menczer Filippo|         34|\n| Flammini Alessandro|         18|\n|       Kooti Farshad|          4|\n|          Varol Onur|         10|\n|     Burghardt Keith|          5|\n+--------------------+-----------+\n\nthere are 3642 clusters in total\n+---------------+-----------+\n|             id|article_num|\n+---------------+-----------+\n|       Liu Yang|         75|\n|  Zhang Tianwei|         18|\n|          Li Bo|         50|\n|       Chen Sen|         10|\n|     Xue Minhui|         17|\n|         Ma Lei|         20|\n|Juefei-Xu Felix|         12|\n|    Xie Xiaofei|         16|\n|    Huang Yihao|          6|\n|       Guo Qing|          6|\n+---------------+-----------+\n\nthere are 2389 clusters in total\n+------------------+-----------+\n|                id|article_num|\n+------------------+-----------+\n|Krauthgamer Robert|         53|\n|Braverman Vladimir|         32|\n|        Price Eric|         36|\n| Woodruff David P.|        103|\n|         Zhang Qin|         24|\n|             Li Yi|         16|\n|     Musco Cameron|         36|\n|       Zhou Samson|         25|\n|         Song Zhao|         48|\n|    Jayaram Rajesh|         13|\n+------------------+-----------+\n\nthere are 1484 clusters in total\n+-------------------+-----------+\n|                 id|article_num|\n+-------------------+-----------+\n|      Madden Samuel|         32|\n|         Kraska Tim|         44|\n|Franklin Michael J.|         20|\n|      Kemper Alfons|         13|\n|    Gadepally Vijay|         32|\n|      Kepner Jeremy|         32|\n|      Tatbul Nesime|          6|\n|     Binnig Carsten|         16|\n|        Marcus Ryan|         14|\n|       Kipf Andreas|         13|\n+-------------------+-----------+\n\nthere are 972 clusters in total\n+------------------+-----------+\n|                id|article_num|\n+------------------+-----------+\n|  Leydesdorff Loet|        130|\n|     Rafols Ismael|         15|\n|     Bornmann Lutz|        134|\n|       Marx Werner|         22|\n|    Rotolo Daniele|          8|\n|Wagner Caroline S.|         13|\n|     Mutz Ruediger|         10|\n|   Wagner Caroline|          6|\n|  Haunschild Robin|         38|\n|      Thor Andreas|          7|\n+------------------+-----------+\n\nthere are 1846 clusters in total\n+---------------+-----------+\n|             id|article_num|\n+---------------+-----------+\n|  Fici Gabriele|         28|\n|Rampersad Narad|         11|\n|Shallit Jeffrey|         52|\n| Schaeffer Luke|          7|\n|Currie James D.|          6|\n|     Goc Daniel|          3|\n| Mousavi Hamoon|          4|\n|  Gabric Daniel|          8|\n|  Clokie Trevor|          2|\n|      Mol Lucas|          8|\n+---------------+-----------+\n\nthere are 3224 clusters in total\n+--------------------+-----------+\n|                  id|article_num|\n+--------------------+-----------+\n|      Buyya Rajkumar|         86|\n|Calheiros Rodrigo N.|         13|\n|    Varghese Blesson|         54|\n|Dastjerdi Amir Vahid|          6|\n|          Xu Minxian|         14|\n|        Bahsoon Rami|          8|\n|  Gill Sukhpal Singh|          9|\n| Toosi Adel Nadjaran|          8|\n|  Rodriguez Maria A.|          8|\n|       Tuli Shreshth|          8|\n+--------------------+-----------+\n\n"}], "source": "for category in categories:\n    rdd_test = sqlcontext.read.json('gs://bigdata-homework2-question2-bucket/arxiv_data_cs_field_category_{}.json'.format(category))\n    authors = rdd_test.select('authors').rdd\n\n    vertices = authors.flatMap(lambda x: [i for i in x[0]]).map(lambda x: (x,1)).reduceByKey(lambda x, y: x+y)\n    v=vertices.toDF(['id','article_num'])\n\n    authors_pairs = authors.map(lambda x:list(permutations(x[0],2)))\n    edges = authors_pairs.flatMap(lambda x:[i for i in x])\n    e = edges.toDF(['src', 'dst'])\n\n\n    g = GraphFrame(v, e)\n    result = g.connectedComponents()\n    \n    cluster_amount = result.select('component').distinct().count()\n    print(\"there are %d clusters in total\" % cluster_amount)\n    \n    pg_result = g.pageRank(resetProbability=0.15,tol=0.01)\n    userrank = pg_result.vertices.select('id','pagerank').orderBy('pagerank',ascending = False).head(10)\n\n    ##component_id = result.where(result['id']==userrank[0].id).select('component').collect()[0].component\n    local_result = g.pageRank(resetProbability=0.15, maxIter=20, sourceId=userrank[0].id)\n    local_userrank = local_result.vertices.select('id','pagerank').orderBy('pagerank',ascending = False).head(10)\n    local_userid = [row.id for row in local_userrank]\n\n    #g2 = g.filterVertices(\"id='Levine Sergey'\")\n    g2 = g.filterVertices(\"id in {}\".format(tuple(local_userid)))\n\n\n    #(lambda x:any(ele in x['categories'] for ele in categories)==True))\n\n    g2.vertices.show()\n    \n    nodes_list = [i for i in local_userid]\n    with open('nodes_table_{}.csv'.format(category),'w',newline='') as f:\n        write = csv.writer(f)\n        write.writerow(['node'])\n        write.writerow(nodes_list)\n\n    edge_df = g2.edges.rdd.map(lambda x:(local_userid.index(x[0]),local_userid.index(x[1]))).toDF(['source','target'])\n    edge_df.toPandas().to_csv('edges_table_{}.csv'.format(category),header=True,index=False)"}, {"cell_type": "code", "execution_count": 38, "id": "db209fcc", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "c282080b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "1f3bce91", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "c7a3f3ae", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f6d219bd", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 5}